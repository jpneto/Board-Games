<html>

<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta http-equiv="Content-Language" content="en-us">
<meta name="GENERATOR" content="Microsoft FrontPage 6.0">
<meta name="ProgId" content="FrontPage.Editor.Document">
<meta name="Microsoft Theme" content="ricepapr 011">
<title>LUDAE</title>
</head>

<body background="ricebk.jpg" bgcolor="#FFFFFF" text="#000000" link="#666633" vlink="#333366" alink="#990000">
<table border="0" width="100%">
  <tr>
    <td width="29%"></td>
    <td width="60%">
      <p align="center"><b><a href="index.htm"><img border="0" src="icons/ludae-logo.gif" alt="The LUDAE project" width="32" height="32"></a></b></td>
    <td width="11%">
      <p align="right"><b>&nbsp;<a name="top"></a></b><a href="index.htm"><img border="0" src="icons/center.gif" alt="goto main page" width="12" height="12"></a>
      <a href="academy.htm"><img border="0" src="icons/next.gif" alt="next..." width="12" height="12"></a><img border="0" src="icons/line.gif" width="160" height="10"></td>
  </tr>
</table>
<h1 align="center"><b>The
LUDÆ project</b></h1>
<h2><b>Introduction</b></h2>
<blockquote>
<table border="0" width="702">
  <tr>
    <td width="476">
      Herein, the main investigational problem is learning.
      This project
  focus on how a system based on a hybrid attitude between total competition and
  total cooperation, might be able to extract knowledge in order to improve its
  own performance over a specific working problem: <i>Learn how to play a
  given game well.</i>
    </td>
    <td width="36">
    </td>
      <td width="170">
        <div align="right">
        <table border="1" width="100%" bgcolor="#99CCFF" bordercolorlight="#333366" bordercolordark="#000000">
          <tr>
            <td width="33%" background="file://Titanic/jpn/public_html/labmag/icons/back-chip2.jpg"><font color="#000099"><b><a href="#related">
        Related Works</a><br>
              <a href="#terminology">
        Terminology</a><br>
              <a href="#arenas">
        Arena &amp; Academy</a><br>
              <a href="#players">
        Players</a><br>
              <a href="#measures">
        Game Measures</a><br>
              <a href="#thanks">Acknowledges</a></b></font></td>
          </tr>
        </table>
        </div>
    </td>
  </tr>
</table>
<h3>Why games?</h3>
  <p>Games are noise-free, cheap and easily
replicable environments where the
concepts of tactics, strategy, searching, learning and other relevant features
of Artificial Intelligence (AI) can be tested in their purest state. Games, due
to their conceptual flexibility, allow an open testing field for many different
ideas.</p>
<p>Games are also fun. They are spread by all cultures, regions and epochs, and might
have been (and continue to be) very important to our evolution, since they provided an
entertainment dimension over intellectual and rational assignments. A game may
be a tool for learning how to think properly.</p>
<h3>What
games?</h3>
<p>Games embark on an ocean of ideas, concepts, strategies and attention from many
different people. In fact, anybody trying to collect all available games, is rapidly overwhelmed
by their number and variety. Here, when the word game is used, it refers to
games <i>without chance</i> (no dice or whatever stochastic gadget) and with <i>no
hidden information </i>(everybody knows the same thing about the game at any
time). Games of this type are usually called <b>perfect-information games</b>.
To restrict even more this still huge set of possible games, we should deal with
games just for 2 players (without the interesting, but complex problems of
Diplomacy, Bluffing, Politics, Alliances, ...), played on regular discrete grids (namely, square and
hexagonal grids) and that are turn-based (this is a direct implication of
perfect-information). More, the game theme (if any) is of no concern, and so
this set of games also fit into the <b>abstract games </b>category.</p>
<p>From all these, we shall focus on games with only one or two types of pieces;
this will exclude Chess (the Drosophila of AI, as Alexander Kronrod said once)
but still including Checkers, Othello, Go, Tic-Tac-Toe and many dozens more (for
a extensive list of such games, go to <a href="http://www.di.fc.ul.pt/~jpn/gv">The
World of Abstract Games</a>). However, this will not imply a severe restriction
on the project (the maximum number of piece types will easily be extended to
more than just 2) but rather to keep the project goals as focused as possible in
the fundamental problem of learning, that we intend to study.</p>
</blockquote>
<p align="center"><img border="0" src="icons/line.gif" width="360" height="10" align="middle"></p>
<h2><a name="related"></a>Other
Works</h2>
<blockquote>
  <p>The actual field of investigation on board game software players includes
  several different approaches. Some of them are reviewed here (also check an
  extensive <a href="http://satirist.org/learn-game/lists/papers.html">list</a>
  of papers from the <a href="http://satirist.org/learn-game/">Machine Learning
  in Games</a>):</p>
  <p>The <i>specific knowledge-based </i>approach has had great achievements in
  some games, the most famous case being IBM's Deep Blue victory against
  Kasparov on the game of Chess. The <a href="http://www.cs.ualberta.ca/~games/">GAMES</a>
  group of the <a href="http://www.cs.ualberta.ca/">University of Alberta</a>
  works on several world-class software's like <a href="http://www.aaai.org/Pathfinder/html/checkers.html">Checkers</a>,
  Hex or Lines-of-Action. <a href="http://www.cs.vu.nl/~victor/thesis.html"> Victor
  Allis</a>' work solved Gomoku, Connect4 and Qubic. <a href="http://www.cs.vu.nl/~john/">John Romein</a>'s <a href="http://www.cs.vu.nl/~john/thesis/index.html">Multigame</a>
  is a program using a rule-oriented language to define how to play, but still requires an human coded
  evaluation for each given game.</p>
  <p>The <i>meta-player</i> approach tries to find known patterns from the rules
  of a given game, and applies them by tuning the set of features that defines the
  evaluation function. <a href="http://ic-www.arc.nasa.gov/ic/projects/Executive/team/barney/bdp.html">Barney
  Pell</a>'s <a href="http://ic-www.arc.nasa.gov/ic/projects/Executive/team/barney/fall93/Mospaper.html">Metagame</a>
  is a program that analyses the rules stated on a specific language, and
  automatically creates a proper evaluation for each game position.&nbsp;The most
  successful system using this approach is the commercial application <a href="http://www.zillionsofgames.com">Zillions
  of Games</a>. These two systems do have very
  distinct performances, they may have very good results for games that are fit for
  their meta-evaluation function, and play very poorly for other cases.&nbsp;</p>
  <p>The <i>learning </i>approach gives the least possible amount of specific
  knowledge for a certain game, and uses mechanisms of optimization to improve
  game play. Some of them insert a lot of knowledge into the set of
  features, and the learning is reduced to the optimization of relative weights
  for those features.&nbsp;Examples of this approach are Samuel's self-learning
  player of the 50's and 60's dealing with Checkers, and Michael Gherrity's <a href="http://satirist.org/learn-game/systems/sal.html">SAL</a>
  [<a href="http://www.gherrity.org/thesis.ps.gz">thesis</a>].&nbsp;</p>
  <blockquote>
  <p> Another
  possibility is to leave that
  task to the program, instead of inserting an expertise account of good
  features (even if the relevance of each feature is left undefined). A consequence is that unexpected features for a given
  game may appear, providing an extra dimension of knowledge acquisition. Related
  work can be found in Levinson's <a href="http://citeseer.nj.nec.com/292782.html">Morph
  II</a>, <a href="http://www.cs.umass.edu/~utgoff/">Utgoff</a>'s <a href="http://citeseer.nj.nec.com/utgoff96elf.html">ELF</a>,
  <a href="http://www.cs.ualberta.ca/~mburo/">Buro</a>'s <a href="http://citeseer.nj.nec.com/buro99from.html">GLEM</a>,
  and <a href="mailto:fawcett@nynexst.com"> Fawcett</a>'s
  Zenith [<a href="ftp://ftp.cs.umass.edu/pub/techrept/techreport/1993/UM-CS-1993-049.ps">thesis</a>].&nbsp;</p>
  <p>Yet another approach was given
  by <a href="http://www.hunter.cuny.edu/cs/Faculty/Epstein/public_html/index.html">Susan
  Epstein</a>'s <a href="http://www.hunter.cuny.edu/cs/Faculty/Epstein/public_html/html/hoyle.html">HOYLE</a>,
  a program that decides its next move based on a previous agreement among several heuristics,
  each one with a certain concern in mind (each heuristic may have different ways to
  compute its knowledge).</p>
  </blockquote>
  <p>The <i>evolutionary</i>  or <i>A-Life</i> approach attacks the problem by using the genetic paradigm to evolve
  decisional structures, which from generation to generation, hopefully improve
  their way to play a game. A good example is David Fogel's <a href="http://www.amazon.com/exec/obidos/ASIN/1558607838/ref=pd_sim_books/002-0444413-2688013">work</a>
  on evolving neural networks for playing Checkers.&nbsp;Another related work
  (using a special neural network architecture) is <a href="http://citeseer.nj.nec.com/lubberts01coevolving.html">SANE</a>,
  where two different populations co-exist, the hosts trying to improve their
  game strength while a population of parasites improve in order to defeat the
  hosts, providing a constant &quot;aggressive&quot; background from where the
  hosts must evolve. Another game tried with evolving networks is <a href="http://citeseer.nj.nec.com/weaver96evolution.html">Dots'n'Boxes</a>.</p>
  <p>General information can be found in the next links [<a href="http://www.xs4all.nl/~verhelst/chess/research.html">1</a>,
  <a href="http://www.brl.ntt.co.jp/people/kojima/links/ai-game.html">2</a>].
  Check also the <a href="http://www.cs.rulimburg.nl/~uiterwyk/cg.htm">Computer
  Games Group</a>, in the Maastricht Univ. - Netherlands, and the <a href="http://www.cs.inf.shizuoka.ac.jp/~iida/CGRI/CGRI.html">Computer
  Games Research Institute</a> at Shizuoka Univ. - Japan.</p>
</blockquote>
<p align="center"><img border="0" src="icons/line.gif" width="360" height="10" align="middle"></p>
<h2><a name="terminology"></a>Terminology</h2>
<blockquote>
  <p align="right"><i>A scientist would rather use someone else's
  toothbrush&nbsp;<br>
  than another's scientist's nomenclature.<br>
  </i><font size="2">old saying</font></p>
  <p>Inside LUDÆ, there will be a set of agents, called <b>players</b>, able to
  perform several <b>contests</b> of a given <b>game</b>. A <b>tournament</b> is
  a set of contests between a set of players, where all players play against the
  others.&nbsp;</p>
  <p>A game consists of a set of gaming material (the <b>board</b> and the <b>piece
  set</b>), the <b>rule set</b>, i.e., a set of rules<b> </b>(which defines the
  set of valid <b>moves</b> for a given game <b>state</b>), the <b>endgame
  function </b>(a function that defines when the game ends) and an
  initial <b>setup</b> (i.e., the initial state for every contest). A game state
  is defined by a certain board position, the next player and some extra
  information (e.g., the number of off-board pieces, the actual game phase,
  number of captured pieces, ...) visible to both players. More formally:</p>
<table border="0" width="717">
  <tr>
    <td width="709">
      <ul>
		<li>The
            board is a set of labels, or <b>cells</b>, and a set of ordered pairs of
            cells, or <b>links</b>
            <ul>
				<li>Each
                  cell is associated with a value, which defines if it's empty
                  or occupied by a certain piece&nbsp;(white/black soldier/king)</li>
				<li>A
                  board may, optionally, have labels associated with set of
                  cells, called <b>sectors</b>.</li>
				<li>A
                  <b>position </b>is a board with a set of values associated
                  for each cell</li>
		</ul>
		</li>
		<li>A
            state consists of:
		<ul>
			<li>A
                  board position</li>
			<li>Which
                  is the next player</li>
			<li>An optional set of
                  <b>registers</b> defining extra information.</li>
		</ul>
		</li>
		<li>The
            set <i>S</i>
            is the set of all states for game G.
		<ul>
			<li>S<sub>0</sub>
                  is a special state called the <b>game setup</b></li>
		</ul>
		</li>
		<li>The
            <b>
            rule set</b> <i>R</i> is a relation SxS
		<ul>
			<li>Each
            pair (S<sub>i</sub>, S<sub>j</sub>) in R is called a <b>legal</b> <b>move</b></li>
			<li>S
            is a <b>valid</b> <b>state</b> iff&nbsp; S<sub> </sub>= R<sup>n</sup>(S<sub>0</sub>),
            i.e., if there is a set of n&gt;=0 legal moves that reaches S from
            the setup</li>
			<li>All
                  valid states S<sub>F</sub> from which there are no (S<sub>F</sub>,
                  S) in R are called <b>final states</b>.</li>
			<li>A
                  game allows passes if R is reflexive</li>
			<li>A
                  game is invertible if R is symmetric</li>
			<li>A
                  game has a super-KO rule if R is transitive</li>
		</ul>
		</li>
		<li>The
            <i>
            endgame</i> function: S <b>-&gt;</b> NxN&nbsp; is defined has:
		<ul>
			<li><i>endgame</i>(
                  S )
                  = &lt;0,0&gt;&nbsp; if S is not a final state</li>
			<li><i>endgame</i>(
                  S )
                  = &lt;1,1&gt;&nbsp;&nbsp;if S is a final state and a win for the
                  first player</li>
			<li><i>endgame</i>(
                  S )
                  = &lt;1,0&gt;&nbsp; if S is a final state and a draw for both player</li>
			<li><i>endgame</i>(
                  S )
                  = &lt;1,-1&gt; if S is a final state and a win for the
                  second player</li>
		</ul>
		</li>
		<li>The
            rule set R is also called the <b>game tree</b>
            <ul>
				<li>The
                  set of legal moves starting at S<sub>0</sub> which are known
                  and considered as good or bad moves, is called the <b>opening theory</b>
                  of the game</li>
				<li>The
                  set of valid states from which it is known all legal moves
                  until some final state is reached, is called the <b>endgame
                  theory</b> of the game</li>
				<li>The
                  set of all other valid states is called the <b>middlegame</b>.</li>
		</ul>
		</li>
		<li>An
            <b>action</b> is a function S-&gt;S.&nbsp;
		<ul>
			<li>An
                  action is an atomic change on the game board (e.g.,
                  insert/remove a piece or cell, change a game register, ...).</li>
			<li>A
                  set of actions A<sub>1</sub>, A<sub>2</sub>, ..., A<sub>n</sub>
                  define a legal move for a given state S, iff&nbsp; (S, A<sub>1</sub>oA<sub>2</sub>o...oA<sub>n</sub>(S))
                  belongs to R</li>
		</ul>
		</li>
		<li>A
            <b>game policy</b> is a function P:S-&gt;A<sup>n</sup> that decides
            for each valid state, what should be the next actions.
		<ul>
			<li>A
                  <b>tactic</b> is an utility function returning a real for each
                  valid state.</li>
			<li>A
                  <b>strategy</b> is a
weighted sequence of tactics.</li>
			<li>The
                  policy then uses one or more strategies to maximize the
                  expected value of the next state, and so to decide which will
                  be the next move, i.e., P(state) = arg max<sub>actions</sub> (strategy(actions(state)))</li>
		</ul>
		</li>
		</ul>
    </td>
  </tr>
</table>
  <p>In most games, the rule set R cannot be stated explicitly (the game tree may
be infinite or extremely large), so there must be a implicit description to
define R. A <i>low-level description</i> focuses on coding the procedure <i>validMoves</i>(<i>state</i>),
which gives the set of valid moves from a given legal state. A <i>high-level
description</i> focuses on defining a language to represent rules (<a href="http://www.zillionsofgames.com">Zillions</a>'
<a href="http://home.t-online.de/home/j_markmann/zrf.html">ZRF</a> is the most
used and successful example).&nbsp;</p>
<p align="center"><img border="0" src="icons/line.gif" width="360" height="10" align="middle"></p>
</blockquote>
<h2><a name="arenas"></a>Arenas
&amp; Academies</h2>
<blockquote>
  <p>LUDÆ is a system which tries to produce knowledge between cooperation and
  competition. This is the basis for the two main structures, the <b>Arena</b>
  and the <b>Academy</b>. The Arena is where all tournaments occur; the Academy
  is where all knowledge collected by all previous tournaments is kept and made
  accessible to all existing players for learning and improvement.</p>
  <p>Inside the Arena, at each tournament, there is a set of players. These
  players may have different ways of decision making (decide which is the best
  move among all possible options) and learning (update the parameters of the
  decision structure). This plethora of different attitudes of playing a game
  seems useful to enhance the global performance of the system. In any
  optimization problem, the avoidance of local minima is a main concern to
  achieve better results. With the use of different searching strategies, some
  local minima may be easier to escape by some players than others, who will,
  eventually, be able to send out that information to others, via the Academy.</p>
  <p>After a fixed number of tournaments (or just one tournament, if there is no
  learning involved), the Arena activates a selection mechanism, favoring
  statistically the best performances. The next generation of players will be
  found among those best entries of the previous tournament, plus some new
  players that are genetic combinations and mutations of them (at least, for those players to whom
  the combination or mutation of their internal structures makes sense).</p>
  <p>The Academy is populated with knowledge gathered by those players. There
  are many open questions in this subject:</p>
<table border="0" width="718">
  <tr>
    <td width="710">
      <ul>
		<li>What
            should be the criteria to insert/remove knowledge?&nbsp;
		<ul>
			<li>Drastic
                  increase/decrease of performance after some change in the
                  player's control structure</li>
			<li>How
                  to quantify the quality of each piece of knowledge? (the number of uses,
                  the number of successes, in this case, the value of a new
                  tactic should be proportional to victory increment)</li>
			<li>A
                  forgetting value for unused knowledge, updated on each
                  generation</li>
		</ul>
		</li>
		<li>What
            type of knowledge can be kept?
		<ul>
			<li>The
                  opening theory, or Fuseki</li>
			<li>The
                  endgame theory
			<ul>
				<li>Know
                        patterns should be merged if possible, to achieve more
                        generalized ones.</li>
			</ul>
			</li>
			<li>Local/middle
                  game pattern moves (tactics), or Tesujis
			<ul>
				<li>Use
                        patterns to represent them?&nbsp;</li>
				<li>Does
                        it make sense to search for regular piece patterns when
                        analyzing a contest? (e.g., is it good to have a T
                        pentomino on game G?)</li>
				<li>How
                        to extract Tesujis from expert games?</li>
				<li>How
                        to generalize two similar Tesujis?</li>
			</ul>
			</li>
			<li>Feature
                  evaluators (heuristics) and decision mechanisms (strategies)</li>
			<li>All
                  players for historical or even genetic purposes (?)</li>
		</ul>
		</li>
		<li>What
            is the representational language used?
		<ul>
			<li>A
                  language able to be translated to and from the players own
                  languages (an Esperanto)</li>
			<li>All
                  knowledge is the same, or is different?</li>
		</ul>
		</li>
		<li>Can
            expert Human knowledge be added online?</li>
		<li>Can
            knowledge be mixed? How?</li>
		</ul>
    </td>
  </tr>
</table>
</blockquote>
<div align="center">
  <center>
  <table border="0" width="600">
    <tr>
      <td width="45%">
<h3 align="center"><b>THE ACADEMY</b></h3>
        <p><a href="academy.htm"><img border="0" src="icons/academy.gif" width="276" height="206" alt="Rafael Sanzio - The School of Athens, 1500s"></a></td>
      <td width="10%"></td>
      <td width="45%">
<h3 align="center"><b>THE ARENA
</b></h3>
        <p><a href="arena.htm"><img border="0" src="icons/arena.gif" alt="The Colisseum of Ancient Rome" width="308" height="206"></a></td>
    </tr>
  </table>
  </center>
</div>
<p align="center"><img border="0" src="icons/line.gif" width="360" height="10" align="middle"></p>
<h2><a name="players"></a>Players</h2>
<blockquote>
  <p>A player is able to perform a set of contests for a
  certain game. It is made of:&nbsp;</p>
<table border="0" width="731">
  <tr>
    <td width="723">
      <ul>
		<li>A
            communication module to talk with the Arena (and thus, with
            other players)</li>
		<li>A
            communication module to talk with the Academy (to send/receive
            commands)
		<ul>
			<li>Needs
                  to know how to translate his knowledge to the Academy language</li>
		</ul>
		</li>
		<li>A
            search mechanism over the game tree (<a href="http://www.seanet.com/~brucemo/topics/alphabeta.htm">alpha-beta</a>, ...)</li>
		<li>A
            game policy
		<ul>
			<li>The
                  policy internal structure is defined by the player's type&nbsp;(it
                        may use genes, neural nets, expert features, ...)</li>
		</ul>
		</li>
		</ul>
    </td>
  </tr>
</table>
<p>A player should:</p>
<table border="0" width="727">
  <tr>
    <td width="719">
      <ul>
		<li><i>know
            the game rules</i> in order to create the set of valid moves for a
            given state (i.e., it must be able to create the game tree if given
            sufficient time)</li>
		<li><i>be
            able to decide </i>between the options he has.</li>
		<li><i>(optionally)
            be able to learn</i>, so that it can (hopefully) improve its own
            ability to decide</li>
		<li><i>(optionally)
            be able to analyze </i>the game tree, independently of actual game
            experience.</li>
		</ul>
    </td>
  </tr>
</table>
<p>The last two points refer to opposite ways to gather data. Analysis is an <i>active
exploration </i>of the game tree, which produces knowledge without any
experience, in response to eventual game states. Learning is <i>passive
exploration</i> of the game tree, which produces knowledge in response to occurring
game states. In principle, both methods could gather the same information if
sufficient resources were allocated, but each has its weaknesses. Even a deep
analysis of the game tree may not be sufficient to find specific problems that
only learning experience against an expert can come upon. <font face="Times New Roman, Times">On
the other hand, learning by making a bad move that cannot be reverted is worse,
if the player would have been able to predict it and avoid it in the first
place.</font> Analysis can
create useless knowledge about situations that will only occur very rarely in
real gaming. Learning only from games against novice players, could be bad in
the long term.</p>
<p>When speaking of learning, the first two points above are included in what is
called the <i>weak theory of game domains </i>(check Epstein's work for more
details). In principle, a learning player would not even have to know the rules,
it could play randomly at the beginning and learn the game tree (i.e., the set
of legal moves) by experience. However, this is a major waste of computer
resources and a questionable way to learn a game (no human learn a game this
way). A more relevant point is to decide what else a player should know.
Should it know some &quot;general&quot; heuristics (like mobility, capturing
potential to compute piece value)? And Alpha-Beta? Specific game heuristics (e.g., corner cells are
good in Othello)?</p>
  <p>In principle, a weak theory should give the minimum and most general
knowledge in order to support the acquisition of valuable and specific
knowledge. Learning should be helped by this information, but not directed by
it. In this framework, the player should learn how to use the given weak theory of
meta-games for the specific game it plays. But computational resources are
bounded (too bounded for most cases) and a compromise must be achieved between
the minimum theoretical information (i.e., no information at all) and the
minimum possible without destroying the player abilities. [<i>This is still a very
open question, at the present initial stage of LUDÆ</i>]&nbsp;</p>
<p>An important point is the following. Until now, the related works (mentioned
early) have not mixed active and passive game tree exploration; analysis and
learning do not cooperate in building the player's game strength. If a
high-level description of the rules is available (even if it is harder to
implement and has slower execution), it gives the player the possibility to
check some game properties (e.g., piece power or goal type), and this knowledge
can be used to adapt the learning structure, before the learning even begins!
This attitude of planning before start playing, probably implies a more effective information acquisition.
How much pre-built knowledge should be inserted on the making of this planning
structure is a very relevant and (as far as I know) a new question (the answer
should on the minimum possible expert information, without seriously
compromising performance).</p>
<p>If a high level description is not available, the player can even so, be
able to analyze the next features:</p>
<table border="0" width="693">
  <tr>
    <td width="685">
      <ul>
		<li>The
            board topology (number of cells, average links per cell, ...).</li>
		<li>The
            average number of moves for each piece type; this will give an
            expected piece value.</li>
		<li>Is
            the game symmetric? By analyzing the setup and random positions
            while applying <i>validMoves</i>(), it would be possible to achieve
            a high degree of certainty about this feature. But this should be a
            part of the weak theory domain.&nbsp;
		<ul>
			<li>In
                  this case, the player should use two distinct evaluation
                  functions (one for each color).</li>
		</ul>
		</li>
		</ul>
    </td>
  </tr>
</table>
	<p>The player decision mechanism, also called
<b>policy</b>, is based on strategies, i.e., a weighted sequences of tactical
evaluations (or just tactics). The better
player has a better policy, even if it may have the same tactical knowledge of
a worse player (it is the way he chooses from them, that's better). An <b>optimal
policy</b> identifies a path from the initial game state to the maximum final
state value achievable, taken a certain opponent's behavior. A possible relevant
point is that, even in symmetric games, it is known that perfect play leads to
the victory of first or second player (or ending in a draw). Does this mean that players should have
two different strategies, one when playing first, and another when playing
second?</p>
<p>Mark Thomson's comment: <span lang="EN-US" style="font-size:12.0pt;mso-bidi-font-size:
10.0pt;font-family:&quot;Times New Roman&quot;;mso-fareast-font-family:&quot;Times New Roman&quot;;
mso-ansi-language:EN-US;mso-fareast-language:EN-US;mso-bidi-language:AR-SA"><i>In
a losing game, the player should, ideally, play in such a way that its opponent
has the greatest possible chance of making a mistake and losing his advantage.
That would involve inferring the other player’s strategy, and setting up a
situation where it would lead the other player astray. Alternatively, and more
simply, the losing player could choose the move that delays loss for the longest
time, on the assumption that this would tend to give the other player many
chances to make a mistake</i></span></p>
<h3>
<a name="elo"></a>Playing
Strength</h3>
  <p>Each player's strength in a given game, is directly based on the performance
  of its strategy to play that game. How are we to compare different players'
  strengths?
  International game federations use established ratings to classify players by
  their compared performance in actual contests. For example, FIDE Chess <a href="http://gobase.org/rating/elo.html">ELO</a>
  (proposed by Prof. Arpad Elo), uses a <a href="http://ce597n.www.ecn.purdue.edu/CE597N/1997F/students/michael.a.kropinski.1/project/tutorial#Normal Distribution">normal</a>
  distribution and a formula. After a contest, the new ELO for the player is calculated by:</p>
  <blockquote>
    <p>New ELO = Old ELO + C . (Score of the Game - Expected Score)</p>
    <p>where C is a constant that decreases when the player's strength
    increases. Namely, C = 30 for players with ELO less than 2000, C = 10 for
    ELO &gt; 2400, or otherwise C = 130 - (opponent's rating)/20.</p>
  </blockquote>
  <p>The score is, as usual, 0 for a loss, 0.5 for a draw and 1.0 for
  a win. The expected score of a game is given by the normal distribution, with
  mean 0.5 (the value for the draw) and variance of 200. This means that two
  players with a difference of 200 points, the stronger will have an expected
  result of 0.84. A difference of 100 ELO, will result in an expected score of 0.69 for the better player. A difference greater than 426 will not earn ELO
  points for the better player (after rounding the decimal part). A fast <a href="http://www.vog.ru/ratings/elodetails.phtml">way</a>
  to compute this expected value is: Excepted Result = 1 / (1 + 10^(-D/400)),
  where D is the actual ELO difference between players.</p>
  <p>The worst player possible is <b>Random</b>, which strategy is to choose
  randomly one legal move. This player, by definition, has a fixed ELO zero.
  Notice that a player with a misère strategy (i.e., trying to loose) will have
  eventually a negative ELO.&nbsp;</p>
  <p>Let's assume that a strategy S2 is one playing level above strategy S1,
  when the expected score of S2 against S1 is around 2/3 (wins two out of three
  games), i.e., their ELO difference is approx. 100. If another strategy S3 is one
  level above S2, is it possible that the expected score of S3 against S1 is 0.84 (ELO difference is 200)? Is this true? Only if the relation is
  transitive, which it probably is not. Playing strength only makes sense within a
  pool of players in a tournament. In this condition, ELO gives an average of
  the expected score, when compared with the players of that tournament. If the
  number of players is very big - like in Chess or Go worlds - each player is
  only able to perform a restricted number of contests, giving an expected value
  for its ELO.</p>
  <p>A strategy is <b>stable </b>if when facing the same pool of players, its
  ELO is the same within an interval of 50 ELO points; otherwise, it's <b>unstable</b>.&nbsp;</p>
  <p>A move is <b>reasonable</b> for a player with ELO E, if it would be chosen
  by another player with a strategy with ELO &gt; E - 100. A move is <b>good </b>if
  it would be chosen by another player with a strategy with ELO &gt; E + 100.
  Otherwise, it is a <b>poor</b> move. Obviously, this refers to a specific
  tournament's point of view (a good move may be considered bad because of poor
  analysis or just because they are all bad players).</p>
  <p>A tactic, i.e., a weighted feature within a strategy, is <b>essential </b>(<b>harmful</b>),
  if its removal will decrease (increase) the strategy performance by one level
  (ELO increase/decrease of 100). It is <b>critical </b>(<b>destructive</b>), if the
  increase (decrease) is greater than 200 ELO points. A tactic is <b>redundant</b>
  if its removal does not affect the strategy strength. If a tactic removal
  changes the overall strategy strength for every tournament, it's <b>structural</b>.</p>
</blockquote>
<p align="center"><img border="0" src="icons/line.gif" width="360" height="10" align="middle"></p>
<h2><b><a name="measures"></a>Game
Properties</b></h2>
<blockquote>
  <p>This section presents some proposals for qualitative and quantitative
  properties to define and classify abstract games.</p>
  <h3><b>Quantitative
measures</b></h3>
<table border="0" width="701">
  <tr>
    <td width="693">
      <ul>
		<li>The
            dimension of the search space, i.e., the number of possible
            positions inside the game tree, can be used to define the potential
            game complexity.
            <p><b>Size</b>( G ) = log<sub>10</sub> ( N<sub>T</sub> )</p>
            <p>where N<sub>T</sub> is the number of nodes of game tree for G.</p>
            </li>
		<li>The
            potential number of levels of expertise that can be found by
            different strategies.
            <p><b>Depth</b>( G ) = E / 100 + 6 / log<sub>10</sub>( PL )</p>
            <p>where E is the ELO of the best known player, and PL is the
            greater number between 10 or the number of existing human players of
            G. The use of log is two-fold, a crude approximation of the number
            of players is sufficient, and the value added to E, slowly decreases
            as PL increases (the progression of expertise does not seem linear
            with the number of players). The number 6 points that when we get a
            population of one million serious players, the best one should be, approximately,
            one level
            below the perfect player.</p>
            </li>
		<li>The
            branching factor of the game, i.e.,&nbsp;
            <p><b>Width</b>( G ) = The average expected number of legal moves of
            a valid position</p>
            <p>This measure can be statistically approximated by automatic
            search on actual contests.</p>
            </li>
		<li>The
            timing between actual interaction (i.e., the possibility to capture
            or the entering on territory with direct enemy influence) between
            reasonable adversaries.
            <p><b>Tempo</b>( G ) = The average number of moves to piece
            interaction</p>
            <p>This measure can be statistically approximated by game analysis
            of actual contests against players positioned in the top 25%
            ELO.&nbsp;</p>
            </li>
		<li>The
            overall piece mobility
            <p><b>Mobility</b>( G ) = The average (number of moves per piece *
            percentage of empty cells)</p>
            <p>This measure can be statistically approximated by automatic
            search on positions from actual contests.<br>
            </li>
		<li>The
            rules simplicity may also be a measure of a game's quality
            (especially when judged with the game's depth). We can use
            Kolmogorov notion of <a href="http://200.17.161.33/~campani/beje/information.htm#SECTION00036000000000000000">complexity</a>
            in this measure (for games with random elements, the Shannon notion
            of entropy should also be useful).<p><b>Clearness</b>( G ) = The
            size of the smallest programs that compute <i>validMoves</i>( ) and <i>endGame</i>(
            )<p>In this context, the informal notion of a game overall strategy
            may be related with the algorithmic size of the (usually not
            available given bounded resources)
            function <i>findOptimumMove</i>( ). Another point, the precise
            number given by this definition of Clearness is uncomputable (!) We
            are only able to produce an estimation of its true value.</li>
		</ul>
    </td>
  </tr>
</table>
  <h3><b>Qualitative
measures</b></h3>
  <p>These measures deal with
the relevance of the overall player's ignorance about the game tree. Their exact
values would only be computable if the entire game tree analysis were
possible.&nbsp;</p>
<table border="0" width="699">
  <tr>
    <td width="691">
      <ul>
		<li>After
            a positional or material advantage, is it possible to secure that advantage in
            order to win? Or is easy to reverse it, to achieve a draw?
            <p><b>Decisiveness</b>( G ) = The average percentage of positions
            with positional advantage for which there is a winning
            strategy.&nbsp;</p>
            <p>A positional advantage is defined by a positive evaluation using
            the strategy of the best known player. If decisiveness approaches
            zero, it means that the game tree endgame states are densely covered
            with draws. This implies that a winning state can just be achieved
            with the help of both players (i.e., with a bad enemy move), meaning
            that the game is drawish, and thus, flawed.&nbsp;</p>
            <p>A game with good decisiveness should <span lang="EN-US" style="font-size:12.0pt;mso-bidi-font-size:
10.0pt;font-family:&quot;Times New Roman&quot;;mso-fareast-font-family:&quot;Times New Roman&quot;;
mso-ansi-language:EN-US;mso-fareast-language:EN-US;mso-bidi-language:AR-SA">easily
            be able to reach</span> pre-endgame positions, where the positional or material advantage of
            those positions are enough to win the game with reasonable playing.
            Using Go <a href="http://www.uq.net.au/~zzjhardy/japgloss.htm">terminology</a>,
            a sensible sente/gote unbalance should be enough to determine the
            winning outcome of a contest.</p>
            <p>&nbsp;</li>
		<li>After
            a positional advantage, is there any reasonable chance of recovering?
            <p><b>Drama</b>( G ) = The average percentage of reasonable moves
            that lead to a balanced position, given a positional advantage.</p>
            <p>Drama is, in a sense, is the opposite of decisiveness. Poor drama
            leads to quick and decisive action; high drama should cause
            unexpected results, despite any positional superiority. But their
            definitions are not mutually exclusive, games may have excellent
            decisiveness, even though only very good players can profit from it!</p>
            <p>&nbsp;</li>
		<li>How
            easy is it to define a tactical and strategic plan?
            <p><b>Clarity</b>( G ) = the average depth of the sub-tree necessary
            to find a reasonable move / log<sub>10</sub> (the average number of
            nodes of that sub-tree)&nbsp;</p>
            <p>So, clarity tells us about how far a strategy must search down the
            tree to identify reasonable moves. For the same number of nodes, if the
            sub-tree is thinner (i.e., deeper), it means better clarity (less
            lateral analysis is needed). The use of the logarithm is just to
            avoid numbers very close to zero.<p>Using some poetic license, the
            lack of clarity is the fog of war that clouds the abyss of perfect
            strategies.</li>
		</ul>
    </td>
  </tr>
</table>
  <p>Another interesting
property is Topologic Invariance (TI) which relates the game playability with
the varying size of different boards. Perhaps TI is a factor that should be
added to each measure, indicating how they suffer accordingly when the board is
changed.&nbsp;</p>
</blockquote>
<p align="center"><img border="0" src="icons/line.gif" width="360" height="10" align="middle"></p>
<h2><a name="thanks"></a>Acknowledgments</h2>
<blockquote>
  <p><i>I wish to thank <a href="mailto:ploog@mathematik.hu-berlin.de">David
  Ploog</a> for a fruitful discussion on criteria for game classification, and
  to refer the notions of Tempo and Hardness (another name for Decisiveness). There are some <a href="discussions.htm">open discussions</a> about
  these subjects&nbsp;with <a href="discussions.htm#claude">Claude Chaunier</a>, <a href="discussions.htm#lawson">John
  Lawson</a> and <a href="discussions.htm#quielle">Jean-Pierre Queille</a>,
  which contributed a lot to this document.</i>  <i>
  Also a big thanks to<a href="mailto:browne@research.canon.com.au"> Cameron
  Browne</a>  for his insightful thoughts about game tree complexity and to <a href="http://home.flash.net/~markthom/html/abstract_games.html">Mark
  Thomson</a> for his excellent article &quot;<a href="http://home.flash.net/~markthom/html/game_thoughts.html">Defining
  the Abstract</a>&quot; and also for reviewing this text.</i> <i>I also wish to acknowledge
  all the scientific work referred to herein, that helped me with terminology and
  conceptual tools to elaborate LUDÆ.</i></p>
  <p align="left"><i>And I do not want to forget all the seeds ( o o o ) and
  spiders ( x x x ) that populated thousands of email games in the last 10
  years! Finally, long live the <a href="http://www.montypython.net/scripts/petshop.php">Norwegian
  Blue</a><a href="eonet.htm">!</a> </i></p>
	<p align="right"><i>João Pedro Neto (c) 2002</i></p>
</blockquote>

<p align="right">
<img border="0" src="icons/line.gif" width="360" height="10"><br>
<a href="#top">
<img border="0" src="icons/up.gif" alt="goto top" width="12" height="12"></a> <a href="academy.htm"><img border="0" src="icons/next.gif" alt="next..." width="12" height="12"></a></p>

</body>

</html>
